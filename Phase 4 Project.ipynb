{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Project - NLP\n",
    "Julia MÃ¼ller\n",
    "\n",
    "Data Science Flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "#train test split and undersampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#packages for preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "#packages for modeling and feature selection\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, roc_curve, auc\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspection of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/tweets.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          9092 non-null   object\n",
      " 1   Brand/Product  3291 non-null   object\n",
      " 2   Emotion        9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Set display options to show all rows and increase the column width\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#simplify column names\n",
    "df.columns = ['Tweet','Brand/Product','Emotion']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Brand/Product</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebymany @thenextweb wrote about our #hollergram iPad app for #sxsw! http://bit.ly/ieaVOB</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw so I can show them my Sprint Galaxy S still running Android 2.1.   #fail</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Find &amp;amp; Start Impromptu Parties at #SXSW With @HurricaneParty http://bit.ly/gVLrIn I can't wait til the Android app comes out.</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Foursquare ups the game, just in time for #SXSW http://j.mp/grN7pK) - Still prefer @Gowalla by far, best looking Android app to date.</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Gotta love this #SXSW Google Calendar featuring top parties/ show cases to check out.  RT @hamsandwich via @ischafer =&amp;gt;http://bit.ly/aXZwxB</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Great #sxsw ipad app from @madebymany: http://tinyurl.com/4nqv92l</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>haha, awesomely rad iPad app by @madebymany http://bit.ly/hTdFim #hollergram #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I just noticed DST is coming this weekend. How many iPhone users will be an hour late at SXSW come Sunday morning? #SXSW #iPhone</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Just added my #SXSW flights to @planely. Matching people on planes/airports. Also downloaded the @KLM iPhone app, nicely done.</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Must have #SXSW app! RT @malbonster: Lovely review from Forbes for our SXSW iPad app Holler Gram - http://t.co/g4GZypV</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Need to buy an iPad2 while I'm in Austin at #sxsw. Not sure if I'll need to Q up at an Austin Apple store?</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Oh. My. God. The #SXSW app for iPad is pure, unadulterated awesome. It's easier to browse events on iPad than on the website!!!</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Okay, this is really it: yay new @Foursquare for #Android app!!!!11 kthxbai. #sxsw</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Photo: Just installed the #SXSW iPhone app, which is really nice! http://tumblr.com/x6t1pi6av7</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Really enjoying the changes in Gowalla 3.0 for Android! Looking forward to seeing what else they &amp;amp; Foursquare have up their sleeves at #SXSW</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RT @LaurieShook: I'm looking forward to the #SMCDallas pre #SXSW party Wed., and hoping I'll win an #iPad resulting from my shameless promotion.  #ChevySMC</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RT haha, awesomely rad iPad app by @madebymany http://bit.ly/hTdFim #hollergram #sxsw (via @michaelpiliero)</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>someone started an #austin @PartnerHub group in google groups, pre-#sxsw. great idea</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The new #4sq3 looks like it is going to rock. Update for iPhone and Android should push tonight http://bit.ly/etsbZk #SXSW #KeepAustinWeird</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>They were right, the @gowalla 3 app on #android is sweeeeet! Nice job by the team there. #sxsw</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Very smart from @madebymany #hollergram iPad app for #sxsw! http://t.co/A3xvWc6 (may leave my vuvuzela at home now)</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>You must have this app for your iPad if you are going to #SXSW http://itunes.apple.com/us/app/holler-gram/id420666439?mt=8 #hollergram</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Anyone at  #sxsw want to sell their old iPad?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>At #sxsw.  Oooh. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The best!  RT @mention Ha! First in line for #ipad2 at #sxsw &amp;quot;pop-up&amp;quot; Apple store was an event planner #eventprofs #pcma #engage365</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SPIN Play - a new concept in music discovery for your iPad from @mention &amp;amp; spin.com {link} #iTunes #sxsw @mention</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>@mention  - False Alarm: Google Circles Not Coming NowÂÃÃand Probably Not Ever? - {link} #Google #Circles #Social #SXSW</td>\n",
       "      <td>Google</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>VatorNews - Google And Apple Force Print Media to Evolve? {link} #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>@mention  - Great weather to greet you for #sxsw! Still need a sweater at night..Apple putting up &amp;quot;flash store&amp;quot; downtown to sell iPad2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>HootSuite - HootSuite Mobile for #SXSW ~ Updates for iPhone, BlackBerry &amp;amp; Android: Whether youÂÃÂªre getting friend... {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Hey #SXSW - How long do you think it takes us to make an iPhone case? answer @mention using #zazzlesxsw and weÂÃÂªll make you one!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Mashable! - The iPad 2 Takes Over SXSW [VIDEO] #ipad #sxsw #gadgets {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>For I-Pad ?RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by ... {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>#IPad2 's ÂÃÃ·#SmartCoverÂÃÂª Opens to Instant Access - I should have waited to get one! - {link} #apple #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Hand-Held ÂÃÃ·HoboÂÃÂª: Drafthouse launches ÂÃÃ·Hobo With a ShotgunÂÃÂª iPhone app #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>HOORAY RT ÂÃÃ@mention Apple Is Opening A Pop-Up Store In Austin For #SXSW | @mention {link}</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Orly....? ÂÃÃ@mention Google set to launch new social network #Circles today at #sxswÂÃÂ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>wooooo!!! ÂÃÃ@mention Apple store downtown Austin open til Midnight. #sxswÂÃÂ</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Khoi Vinh (@mention says Conde Nast's headlong rush into iPad publishing was a &amp;quot;fundamental misunderstanding&amp;quot; of the platform #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ÂÃÃ@mention {link} &amp;lt;-- HELP ME FORWARD THIS DOC to all Anonymous accounts, techies,&amp;amp; ppl who can help us JAM #libya #SXSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ÂÃ·Â¼ WHAT? ÂÃ·_ {link} ÂÃ£_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>.@mention @mention on the location-based 'fast, fun and future' - {link} (via @mention #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ÂÃÃ@mention @mention #Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #sxswÂÃÂ @mention</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ÂÃÃ@mention @mention talking about {link} - Google's effort to allow users to have open systems #bettercloud #sxswÂÃÂ</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{link} RT @mention &amp;quot;Google before you tweet&amp;quot; is the new &amp;quot;think before you speak.&amp;quot; - Mark Belinsky, #911tweets panel at #SXSW.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{link} RT @mention 1st stop on the #SXSW #Chaos &amp;amp; @mention hunt: Austin Java. Get in the spy game 4 a chance 2 win an iPad!</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{link} RT @mention Those at #SXSW check out the Holler Gram ipad app from @mention  {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@mention  @mention &amp;amp;  @mention having fun at #google [pic] #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>&amp;quot;via @mention : {link} Guy Kawasaki talks 'Enchanted' at SXSW - HE knows his stuff! #books #internet #Apple #sxsw  &amp;quot;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>#futuremf @mention {link} spec for recipes on the web, now in google search: {link}  #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>#OMFG! RT @mention Heard about Apple's pop-up store in downtown Austin? Pics are already on Gowalla: {link} #sxsw #iPad2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>#Smile RT @mention I think Apple's &amp;quot;pop-up store&amp;quot; in Austin would be a lot more interesting if it actually, you know... popped up #sxsw</td>\n",
       "      <td>Apple</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Again? RT @mention Line at the Apple store is insane.. #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Agree. RT @mention Wait. FIONA APPLE is in town??? Somebody kidnap her and put her in a recording studio until she records a new album. #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>At #sxsw? @mention / @mention wanna buy you a drink. 7pm at Fado on 4th. {link} Join us!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>attending @mention iPad design headaches #sxsw {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Boooo! RT @mention Flipboard is developing an iPhone version, not Android, says @mention #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Check out @mention @mention &amp;amp; @mention in line for their iPad 2 in Austin. Power to them! #sxswi #SXSW  {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Check! RT @mention giving added value to location based services needs to battle check-in fatigue #google #pnid #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Chilcott: @mention #SXSW stand talking with Blogger staff. Too late to win competition for best tweet mentioning @mention So no t-shirt.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Do it. RT @mention Come party w/ Google tonight at #sxsw! {link} - Bands, food, art, ice cream, nifty interactive maps!</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Gowalla's @mention promises to launch Foursquare check-in + Groupon rewards-type service at #SXSW. Finger's crossed. {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Ha.ha. RT @mention #SXSW News: Yahoo.com is loosing search traffic to new site, Google.com. Doubt it will last tho w/ that weird name.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Holla! RT @mention At google party. Best ever! Get your butt over here. #sxsw</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>I love my @mention iPhone case from #Sxsw but I can't get my phone out of it #fail</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>I worship @mention {link} #SXSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>iPad2? RT @mention Droid &amp;amp; Mac here :) RT @mention My #agnerd confession, using laptop, iPad &amp;amp; blackberry to follow #SXSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Launching @mention #SxSW? RT @mention @mention Denies Social Network Called Circles Will Debut Today, Despite Report {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>New Post: @mention iPhone app makes it easy to connect on all social networks with people you meet  {link} #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Nice that @mention iPhone app is behaving today. Crashes yesterday were ridiculous. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Nice!  RT @mention Apple opening popup store for iPad launch in downtown Austin during #SXSW {link} via @mention</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Nice!! RT @mention Hey, Apple fans! Get a peek at the space that's slated to be a pop-up #SXSW Apple Store tomorrow: {link}</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>one thing @mention is doing so great is get a great, down to earth face to Google as a company - You can only love her #sxsw #sxwsi</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Stay tune @mention showcase #H4ckers {link} #SXSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Thank you @mention @mention for the #touchingstories preso #SXSW . Here's their deck {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Thank you @mention for an awesome #sxsw party! {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Thanks RT @mention If you're trying to contact friends or family in #Japan, @mention has created a person finder: {link} #SXSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Thanks to @mention for her mention of our new #Speech iPad apps being showcased at the #SXSW Conf. {link} #sxswh #sxsh</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Thanks to @mention for publishing the news of @mention new medical Apps at the #sxswi conf. blog {link} #sxsw #sxswh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I can't tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Thanks to @mention for publishing the news of our new medical Apps in the #sxswi conf. blog {link} #sxsw #sxswh #mhealth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>What !?!? @mention  #SXSW does not provide iPhone chargers?!?  I've changed my mind about going next year!</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Wonder if @mention &amp;amp; @mention will be in the apple flashmob: tcrn.ch/fcs45j #SXSW #ipad2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Wonder if @mention is putting tips from the @mention API... #SxSW #SUxSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>XMAS!! RT @mention Shiny new @mention @mention @mention apps, a new @garyvee book, pop-up iPad 2 stores... #SXSW is Christmas for nerds.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Yai!!! RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by (cont) {link}</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Yes!!! RT @mention hey @mention , i've got another gem for you --&amp;gt; free @mention sxsw {link} #SXSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Fast, Fun &amp;amp; Future: @mention of Google presenting at #sxsw on search, local and mobile</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GSD&amp;amp;M &amp;amp; Google's Industry Party Tonight @mention - See u there! {link} #SXSW #Austin #Welivehere #GSDM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          Tweet  \\\n",
       "0                               .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1                   @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                                               @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                                            @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4                           @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "5                  @teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd   \n",
       "6                                                                                                                                                           NaN   \n",
       "7                    #SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan   \n",
       "8                             Beautifully smart and simple idea RT @madebymany @thenextweb wrote about our #hollergram iPad app for #sxsw! http://bit.ly/ieaVOB   \n",
       "9                                                                      Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear   \n",
       "10                                       Excited to meet the @samsungmobileus at #sxsw so I can show them my Sprint Galaxy S still running Android 2.1.   #fail   \n",
       "11                            Find &amp; Start Impromptu Parties at #SXSW With @HurricaneParty http://bit.ly/gVLrIn I can't wait til the Android app comes out.   \n",
       "12                        Foursquare ups the game, just in time for #SXSW http://j.mp/grN7pK) - Still prefer @Gowalla by far, best looking Android app to date.   \n",
       "13               Gotta love this #SXSW Google Calendar featuring top parties/ show cases to check out.  RT @hamsandwich via @ischafer =&gt;http://bit.ly/aXZwxB   \n",
       "14                                                                                            Great #sxsw ipad app from @madebymany: http://tinyurl.com/4nqv92l   \n",
       "15                                                                           haha, awesomely rad iPad app by @madebymany http://bit.ly/hTdFim #hollergram #sxsw   \n",
       "16                                                                 Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw   \n",
       "17                             I just noticed DST is coming this weekend. How many iPhone users will be an hour late at SXSW come Sunday morning? #SXSW #iPhone   \n",
       "18                               Just added my #SXSW flights to @planely. Matching people on planes/airports. Also downloaded the @KLM iPhone app, nicely done.   \n",
       "19                                       Must have #SXSW app! RT @malbonster: Lovely review from Forbes for our SXSW iPad app Holler Gram - http://t.co/g4GZypV   \n",
       "20                                                   Need to buy an iPad2 while I'm in Austin at #sxsw. Not sure if I'll need to Q up at an Austin Apple store?   \n",
       "21                              Oh. My. God. The #SXSW app for iPad is pure, unadulterated awesome. It's easier to browse events on iPad than on the website!!!   \n",
       "22                                                                           Okay, this is really it: yay new @Foursquare for #Android app!!!!11 kthxbai. #sxsw   \n",
       "23                                                               Photo: Just installed the #SXSW iPhone app, which is really nice! http://tumblr.com/x6t1pi6av7   \n",
       "24             Really enjoying the changes in Gowalla 3.0 for Android! Looking forward to seeing what else they &amp; Foursquare have up their sleeves at #SXSW   \n",
       "25  RT @LaurieShook: I'm looking forward to the #SMCDallas pre #SXSW party Wed., and hoping I'll win an #iPad resulting from my shameless promotion.  #ChevySMC   \n",
       "26                                                  RT haha, awesomely rad iPad app by @madebymany http://bit.ly/hTdFim #hollergram #sxsw (via @michaelpiliero)   \n",
       "27                                                                         someone started an #austin @PartnerHub group in google groups, pre-#sxsw. great idea   \n",
       "28                  The new #4sq3 looks like it is going to rock. Update for iPhone and Android should push tonight http://bit.ly/etsbZk #SXSW #KeepAustinWeird   \n",
       "29                                                               They were right, the @gowalla 3 app on #android is sweeeeet! Nice job by the team there. #sxsw   \n",
       "30                                          Very smart from @madebymany #hollergram iPad app for #sxsw! http://t.co/A3xvWc6 (may leave my vuvuzela at home now)   \n",
       "31                       You must have this app for your iPad if you are going to #SXSW http://itunes.apple.com/us/app/holler-gram/id420666439?mt=8 #hollergram   \n",
       "32                                                          Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}   \n",
       "33                                                                                                                Anyone at  #sxsw want to sell their old iPad?   \n",
       "34                                                                                Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?   \n",
       "35                                                 At #sxsw.  Oooh. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link}   \n",
       "36                The best!  RT @mention Ha! First in line for #ipad2 at #sxsw &quot;pop-up&quot; Apple store was an event planner #eventprofs #pcma #engage365   \n",
       "37                                        SPIN Play - a new concept in music discovery for your iPad from @mention &amp; spin.com {link} #iTunes #sxsw @mention   \n",
       "38                                      @mention  - False Alarm: Google Circles Not Coming NowÂÃÃand Probably Not Ever? - {link} #Google #Circles #Social #SXSW   \n",
       "39                                                                                       VatorNews - Google And Apple Force Print Media to Evolve? {link} #sxsw   \n",
       "40             @mention  - Great weather to greet you for #sxsw! Still need a sweater at night..Apple putting up &quot;flash store&quot; downtown to sell iPad2   \n",
       "41                             HootSuite - HootSuite Mobile for #SXSW ~ Updates for iPhone, BlackBerry &amp; Android: Whether youÂÃÂªre getting friend... {link}   \n",
       "42                            Hey #SXSW - How long do you think it takes us to make an iPhone case? answer @mention using #zazzlesxsw and weÂÃÂªll make you one!   \n",
       "43                                                                                   Mashable! - The iPad 2 Takes Over SXSW [VIDEO] #ipad #sxsw #gadgets {link}   \n",
       "44                                  For I-Pad ?RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by ... {link}   \n",
       "45                                                 #IPad2 's ÂÃÃ·#SmartCoverÂÃÂª Opens to Instant Access - I should have waited to get one! - {link} #apple #SXSW   \n",
       "46                                                                  Hand-Held ÂÃÃ·HoboÂÃÂª: Drafthouse launches ÂÃÃ·Hobo With a ShotgunÂÃÂª iPhone app #SXSW {link}   \n",
       "47                                                                  HOORAY RT ÂÃÃ@mention Apple Is Opening A Pop-Up Store In Austin For #SXSW | @mention {link}   \n",
       "48                                                                     Orly....? ÂÃÃ@mention Google set to launch new social network #Circles today at #sxswÂÃÂ   \n",
       "49                                                                                wooooo!!! ÂÃÃ@mention Apple store downtown Austin open til Midnight. #sxswÂÃÂ   \n",
       "50                Khoi Vinh (@mention says Conde Nast's headlong rush into iPad publishing was a &quot;fundamental misunderstanding&quot; of the platform #sxsw   \n",
       "51                             ÂÃÃ@mention {link} &lt;-- HELP ME FORWARD THIS DOC to all Anonymous accounts, techies,&amp; ppl who can help us JAM #libya #SXSW   \n",
       "52                                                                              ÂÃ·Â¼ WHAT? ÂÃ·_ {link} ÂÃ£_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter   \n",
       "53                                                                 .@mention @mention on the location-based 'fast, fun and future' - {link} (via @mention #sxsw   \n",
       "54                                        ÂÃÃ@mention @mention #Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxswÂÃÂ @mention   \n",
       "55                                        ÂÃÃ@mention @mention talking about {link} - Google's effort to allow users to have open systems #bettercloud #sxswÂÃÂ   \n",
       "56            {link} RT @mention &quot;Google before you tweet&quot; is the new &quot;think before you speak.&quot; - Mark Belinsky, #911tweets panel at #SXSW.   \n",
       "57                              {link} RT @mention 1st stop on the #SXSW #Chaos &amp; @mention hunt: Austin Java. Get in the spy game 4 a chance 2 win an iPad!   \n",
       "58                                                                   {link} RT @mention Those at #SXSW check out the Holler Gram ipad app from @mention  {link}   \n",
       "59                                                                                  @mention  @mention &amp;  @mention having fun at #google [pic] #SXSW {link}   \n",
       "60                               &quot;via @mention : {link} Guy Kawasaki talks 'Enchanted' at SXSW - HE knows his stuff! #books #internet #Apple #sxsw  &quot;   \n",
       "61                                                                   #futuremf @mention {link} spec for recipes on the web, now in google search: {link}  #sxsw   \n",
       "62                                     #OMFG! RT @mention Heard about Apple's pop-up store in downtown Austin? Pics are already on Gowalla: {link} #sxsw #iPad2   \n",
       "63            #Smile RT @mention I think Apple's &quot;pop-up store&quot; in Austin would be a lot more interesting if it actually, you know... popped up #sxsw   \n",
       "64                                                                                                 Again? RT @mention Line at the Apple store is insane.. #sxsw   \n",
       "65                Agree. RT @mention Wait. FIONA APPLE is in town??? Somebody kidnap her and put her in a recording studio until she records a new album. #sxsw   \n",
       "66                                                                     At #sxsw? @mention / @mention wanna buy you a drink. 7pm at Fado on 4th. {link} Join us!   \n",
       "67                                                                                                        attending @mention iPad design headaches #sxsw {link}   \n",
       "68                                                               Boooo! RT @mention Flipboard is developing an iPhone version, not Android, says @mention #sxsw   \n",
       "69                                           Check out @mention @mention &amp; @mention in line for their iPad 2 in Austin. Power to them! #sxswi #SXSW  {link}   \n",
       "70                                        Check! RT @mention giving added value to location based services needs to battle check-in fatigue #google #pnid #sxsw   \n",
       "71                     Chilcott: @mention #SXSW stand talking with Blogger staff. Too late to win competition for best tweet mentioning @mention So no t-shirt.   \n",
       "72                                      Do it. RT @mention Come party w/ Google tonight at #sxsw! {link} - Bands, food, art, ice cream, nifty interactive maps!   \n",
       "73                                  Gowalla's @mention promises to launch Foursquare check-in + Groupon rewards-type service at #SXSW. Finger's crossed. {link}   \n",
       "74                       Ha.ha. RT @mention #SXSW News: Yahoo.com is loosing search traffic to new site, Google.com. Doubt it will last tho w/ that weird name.   \n",
       "75                                                                                Holla! RT @mention At google party. Best ever! Get your butt over here. #sxsw   \n",
       "76                                                                           I love my @mention iPhone case from #Sxsw but I can't get my phone out of it #fail   \n",
       "77                                                                                                                              I worship @mention {link} #SXSW   \n",
       "78                            iPad2? RT @mention Droid &amp; Mac here :) RT @mention My #agnerd confession, using laptop, iPad &amp; blackberry to follow #SXSW   \n",
       "79                                  Launching @mention #SxSW? RT @mention @mention Denies Social Network Called Circles Will Debut Today, Despite Report {link}   \n",
       "80                                             New Post: @mention iPhone app makes it easy to connect on all social networks with people you meet  {link} #sxsw   \n",
       "81                                                                    Nice that @mention iPhone app is behaving today. Crashes yesterday were ridiculous. #sxsw   \n",
       "82                                             Nice!  RT @mention Apple opening popup store for iPad launch in downtown Austin during #SXSW {link} via @mention   \n",
       "83                                  Nice!! RT @mention Hey, Apple fans! Get a peek at the space that's slated to be a pop-up #SXSW Apple Store tomorrow: {link}   \n",
       "84                          one thing @mention is doing so great is get a great, down to earth face to Google as a company - You can only love her #sxsw #sxwsi   \n",
       "85                                                                                                            Stay tune @mention showcase #H4ckers {link} #SXSW   \n",
       "86                                                                  Thank you @mention @mention for the #touchingstories preso #SXSW . Here's their deck {link}   \n",
       "87                                                                                                        Thank you @mention for an awesome #sxsw party! {link}   \n",
       "88                               Thanks RT @mention If you're trying to contact friends or family in #Japan, @mention has created a person finder: {link} #SXSW   \n",
       "89                                       Thanks to @mention for her mention of our new #Speech iPad apps being showcased at the #SXSW Conf. {link} #sxswh #sxsh   \n",
       "90                                         Thanks to @mention for publishing the news of @mention new medical Apps at the #sxswi conf. blog {link} #sxsw #sxswh   \n",
       "91                                     Thanks to @mention for publishing the news of our new medical Apps in the #sxswi conf. blog {link} #sxsw #sxswh #mhealth   \n",
       "92                                                   What !?!? @mention  #SXSW does not provide iPhone chargers?!?  I've changed my mind about going next year!   \n",
       "93                                                                 Wonder if @mention &amp; @mention will be in the apple flashmob: tcrn.ch/fcs45j #SXSW #ipad2   \n",
       "94                                                                                     Wonder if @mention is putting tips from the @mention API... #SxSW #SUxSW   \n",
       "95                     XMAS!! RT @mention Shiny new @mention @mention @mention apps, a new @garyvee book, pop-up iPad 2 stores... #SXSW is Christmas for nerds.   \n",
       "96                                   Yai!!! RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by (cont) {link}   \n",
       "97                                                        Yes!!! RT @mention hey @mention , i've got another gem for you --&gt; free @mention sxsw {link} #SXSW   \n",
       "98                                                                   Fast, Fun &amp; Future: @mention of Google presenting at #sxsw on search, local and mobile   \n",
       "99                                               GSD&amp;M &amp; Google's Industry Party Tonight @mention - See u there! {link} #SXSW #Austin #Welivehere #GSDM   \n",
       "\n",
       "                      Brand/Product                             Emotion  \n",
       "0                            iPhone                    Negative emotion  \n",
       "1                iPad or iPhone App                    Positive emotion  \n",
       "2                              iPad                    Positive emotion  \n",
       "3                iPad or iPhone App                    Negative emotion  \n",
       "4                            Google                    Positive emotion  \n",
       "5                               NaN  No emotion toward brand or product  \n",
       "6                               NaN  No emotion toward brand or product  \n",
       "7                           Android                    Positive emotion  \n",
       "8                iPad or iPhone App                    Positive emotion  \n",
       "9                             Apple                    Positive emotion  \n",
       "10                          Android                    Positive emotion  \n",
       "11                      Android App                    Positive emotion  \n",
       "12                      Android App                    Positive emotion  \n",
       "13  Other Google product or service                    Positive emotion  \n",
       "14               iPad or iPhone App                    Positive emotion  \n",
       "15               iPad or iPhone App                    Positive emotion  \n",
       "16                              NaN  No emotion toward brand or product  \n",
       "17                           iPhone                    Negative emotion  \n",
       "18               iPad or iPhone App                    Positive emotion  \n",
       "19               iPad or iPhone App                    Positive emotion  \n",
       "20                             iPad                    Positive emotion  \n",
       "21               iPad or iPhone App                    Positive emotion  \n",
       "22                      Android App                    Positive emotion  \n",
       "23               iPad or iPhone App                    Positive emotion  \n",
       "24                      Android App                    Positive emotion  \n",
       "25                             iPad                    Positive emotion  \n",
       "26               iPad or iPhone App                    Positive emotion  \n",
       "27  Other Google product or service                    Positive emotion  \n",
       "28               iPad or iPhone App                    Positive emotion  \n",
       "29                      Android App                    Positive emotion  \n",
       "30               iPad or iPhone App                    Positive emotion  \n",
       "31               iPad or iPhone App                    Positive emotion  \n",
       "32                              NaN  No emotion toward brand or product  \n",
       "33                              NaN  No emotion toward brand or product  \n",
       "34                              NaN  No emotion toward brand or product  \n",
       "35                              NaN  No emotion toward brand or product  \n",
       "36                             iPad                    Positive emotion  \n",
       "37                              NaN  No emotion toward brand or product  \n",
       "38                           Google                    Negative emotion  \n",
       "39                              NaN  No emotion toward brand or product  \n",
       "40                            Apple                    Positive emotion  \n",
       "41                              NaN  No emotion toward brand or product  \n",
       "42                              NaN  No emotion toward brand or product  \n",
       "43                              NaN  No emotion toward brand or product  \n",
       "44                              NaN  No emotion toward brand or product  \n",
       "45               iPad or iPhone App                    Positive emotion  \n",
       "46                              NaN                    Positive emotion  \n",
       "47                            Apple                    Positive emotion  \n",
       "48                              NaN  No emotion toward brand or product  \n",
       "49                            Apple                    Positive emotion  \n",
       "50                              NaN  No emotion toward brand or product  \n",
       "51                              NaN  No emotion toward brand or product  \n",
       "52                              NaN  No emotion toward brand or product  \n",
       "53                              NaN  No emotion toward brand or product  \n",
       "54                              NaN  No emotion toward brand or product  \n",
       "55                           Google                    Positive emotion  \n",
       "56                              NaN  No emotion toward brand or product  \n",
       "57                             iPad                    Positive emotion  \n",
       "58                              NaN  No emotion toward brand or product  \n",
       "59                              NaN  No emotion toward brand or product  \n",
       "60                              NaN  No emotion toward brand or product  \n",
       "61                              NaN  No emotion toward brand or product  \n",
       "62                            Apple                    Positive emotion  \n",
       "63                            Apple  No emotion toward brand or product  \n",
       "64                              NaN                    Negative emotion  \n",
       "65                              NaN  No emotion toward brand or product  \n",
       "66                              NaN  No emotion toward brand or product  \n",
       "67                             iPad                    Negative emotion  \n",
       "68                              NaN                    Negative emotion  \n",
       "69                             iPad                    Positive emotion  \n",
       "70                              NaN  No emotion toward brand or product  \n",
       "71                              NaN  No emotion toward brand or product  \n",
       "72                           Google                    Positive emotion  \n",
       "73                              NaN  No emotion toward brand or product  \n",
       "74                              NaN  No emotion toward brand or product  \n",
       "75                           Google                    Positive emotion  \n",
       "76                           iPhone                    Positive emotion  \n",
       "77                              NaN  No emotion toward brand or product  \n",
       "78                              NaN  No emotion toward brand or product  \n",
       "79                              NaN  No emotion toward brand or product  \n",
       "80               iPad or iPhone App                    Positive emotion  \n",
       "81               iPad or iPhone App                    Positive emotion  \n",
       "82                              NaN  No emotion toward brand or product  \n",
       "83                            Apple                    Positive emotion  \n",
       "84                           Google                    Positive emotion  \n",
       "85                              NaN  No emotion toward brand or product  \n",
       "86                              NaN  No emotion toward brand or product  \n",
       "87                              NaN  No emotion toward brand or product  \n",
       "88                              NaN  No emotion toward brand or product  \n",
       "89               iPad or iPhone App                    Positive emotion  \n",
       "90                              NaN                        I can't tell  \n",
       "91                              NaN  No emotion toward brand or product  \n",
       "92                           iPhone                    Negative emotion  \n",
       "93                              NaN  No emotion toward brand or product  \n",
       "94                              NaN  No emotion toward brand or product  \n",
       "95                             iPad                    Positive emotion  \n",
       "96                           iPhone                    Positive emotion  \n",
       "97                              NaN  No emotion toward brand or product  \n",
       "98                           Google                    Positive emotion  \n",
       "99                              NaN  No emotion toward brand or product  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          9092 non-null   object\n",
      " 1   Brand/Product  3291 non-null   object\n",
      " 2   Emotion        9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My dataset has more than 9000 tweets and is split in 3 columns. The first column is the tweet, the second one is the information if the tweet is directed at a specific product (Apple or Google) and the third one is the sentiment towards the product.\n",
    "The second column only contains 3200 data points so we don't know about every of the 9000 tweets at which product they are directed at. Also the 3rd column shows for the majority of tweets no emotion. \n",
    "My next steps are to summarize the different products into the two brands Apple or Google and to check if the missing values in the product column really don't include any information about a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will rename the different products and map them to the brand Apple or Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: Brand/Product, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Brand/Product\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple     2409\n",
      "Google     882\n",
      "Name: Brand, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          9092 non-null   object\n",
      " 1   Brand/Product  3291 non-null   object\n",
      " 2   Emotion        9093 non-null   object\n",
      " 3   Brand          3291 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 284.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "product_mapping = {\n",
    "    \"iPad\": \"Apple\",\n",
    "    \"iPad or iPhone App\": \"Apple\",\n",
    "    \"iPhone\": \"Apple\",\n",
    "    \"Other Apple product or service\": \"Apple\",\n",
    "    \"Other Google product or service\": \"Google\",\n",
    "    \"Android App\": \"Google\",\n",
    "    \"Android\": \"Google\"\n",
    "}\n",
    "\n",
    "\n",
    "df[\"Brand\"] = df[\"Brand/Product\"].replace(product_mapping)\n",
    "print(df[\"Brand\"].value_counts())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, where this is cleaned up, I will look at the na values to see if there are no information connected to Apple or Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5           @teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd\n",
       "6                                                                                                                                                    NaN\n",
       "16                                                          Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw\n",
       "32                                                   Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}\n",
       "33                                                                                                         Anyone at  #sxsw want to sell their old iPad?\n",
       "34                                                                         Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?\n",
       "35                                          At #sxsw.  Oooh. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link}\n",
       "37                                 SPIN Play - a new concept in music discovery for your iPad from @mention &amp; spin.com {link} #iTunes #sxsw @mention\n",
       "39                                                                                VatorNews - Google And Apple Force Print Media to Evolve? {link} #sxsw\n",
       "41                      HootSuite - HootSuite Mobile for #SXSW ~ Updates for iPhone, BlackBerry &amp; Android: Whether youÂÃÂªre getting friend... {link}\n",
       "42                     Hey #SXSW - How long do you think it takes us to make an iPhone case? answer @mention using #zazzlesxsw and weÂÃÂªll make you one!\n",
       "43                                                                            Mashable! - The iPad 2 Takes Over SXSW [VIDEO] #ipad #sxsw #gadgets {link}\n",
       "44                           For I-Pad ?RT @mention New #UberSocial for #iPhone now in the App Store includes UberGuide to #SXSW sponsored by ... {link}\n",
       "46                                                           Hand-Held ÂÃÃ·HoboÂÃÂª: Drafthouse launches ÂÃÃ·Hobo With a ShotgunÂÃÂª iPhone app #SXSW {link}\n",
       "48                                                              Orly....? ÂÃÃ@mention Google set to launch new social network #Circles today at #sxswÂÃÂ\n",
       "50         Khoi Vinh (@mention says Conde Nast's headlong rush into iPad publishing was a &quot;fundamental misunderstanding&quot; of the platform #sxsw\n",
       "51                      ÂÃÃ@mention {link} &lt;-- HELP ME FORWARD THIS DOC to all Anonymous accounts, techies,&amp; ppl who can help us JAM #libya #SXSW\n",
       "52                                                                       ÂÃ·Â¼ WHAT? ÂÃ·_ {link} ÂÃ£_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter\n",
       "53                                                          .@mention @mention on the location-based 'fast, fun and future' - {link} (via @mention #sxsw\n",
       "54                                 ÂÃÃ@mention @mention #Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxswÂÃÂ @mention\n",
       "56     {link} RT @mention &quot;Google before you tweet&quot; is the new &quot;think before you speak.&quot; - Mark Belinsky, #911tweets panel at #SXSW.\n",
       "58                                                            {link} RT @mention Those at #SXSW check out the Holler Gram ipad app from @mention  {link}\n",
       "59                                                                           @mention  @mention &amp;  @mention having fun at #google [pic] #SXSW {link}\n",
       "60                        &quot;via @mention : {link} Guy Kawasaki talks 'Enchanted' at SXSW - HE knows his stuff! #books #internet #Apple #sxsw  &quot;\n",
       "61                                                            #futuremf @mention {link} spec for recipes on the web, now in google search: {link}  #sxsw\n",
       "64                                                                                          Again? RT @mention Line at the Apple store is insane.. #sxsw\n",
       "65         Agree. RT @mention Wait. FIONA APPLE is in town??? Somebody kidnap her and put her in a recording studio until she records a new album. #sxsw\n",
       "66                                                              At #sxsw? @mention / @mention wanna buy you a drink. 7pm at Fado on 4th. {link} Join us!\n",
       "68                                                        Boooo! RT @mention Flipboard is developing an iPhone version, not Android, says @mention #sxsw\n",
       "70                                 Check! RT @mention giving added value to location based services needs to battle check-in fatigue #google #pnid #sxsw\n",
       "71              Chilcott: @mention #SXSW stand talking with Blogger staff. Too late to win competition for best tweet mentioning @mention So no t-shirt.\n",
       "73                           Gowalla's @mention promises to launch Foursquare check-in + Groupon rewards-type service at #SXSW. Finger's crossed. {link}\n",
       "74                Ha.ha. RT @mention #SXSW News: Yahoo.com is loosing search traffic to new site, Google.com. Doubt it will last tho w/ that weird name.\n",
       "77                                                                                                                       I worship @mention {link} #SXSW\n",
       "78                     iPad2? RT @mention Droid &amp; Mac here :) RT @mention My #agnerd confession, using laptop, iPad &amp; blackberry to follow #SXSW\n",
       "79                           Launching @mention #SxSW? RT @mention @mention Denies Social Network Called Circles Will Debut Today, Despite Report {link}\n",
       "82                                      Nice!  RT @mention Apple opening popup store for iPad launch in downtown Austin during #SXSW {link} via @mention\n",
       "85                                                                                                     Stay tune @mention showcase #H4ckers {link} #SXSW\n",
       "86                                                           Thank you @mention @mention for the #touchingstories preso #SXSW . Here's their deck {link}\n",
       "87                                                                                                 Thank you @mention for an awesome #sxsw party! {link}\n",
       "88                        Thanks RT @mention If you're trying to contact friends or family in #Japan, @mention has created a person finder: {link} #SXSW\n",
       "90                                  Thanks to @mention for publishing the news of @mention new medical Apps at the #sxswi conf. blog {link} #sxsw #sxswh\n",
       "91                              Thanks to @mention for publishing the news of our new medical Apps in the #sxswi conf. blog {link} #sxsw #sxswh #mhealth\n",
       "93                                                          Wonder if @mention &amp; @mention will be in the apple flashmob: tcrn.ch/fcs45j #SXSW #ipad2\n",
       "94                                                                              Wonder if @mention is putting tips from the @mention API... #SxSW #SUxSW\n",
       "97                                                 Yes!!! RT @mention hey @mention , i've got another gem for you --&gt; free @mention sxsw {link} #SXSW\n",
       "99                                        GSD&amp;M &amp; Google's Industry Party Tonight @mention - See u there! {link} #SXSW #Austin #Welivehere #GSDM\n",
       "100                     New buzz? &quot;@mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} rt @mention #sxsw&quot;\n",
       "102                      ÂÃÃ@mention &quot;Apple has opened a pop-up store in Austin so the nerds in town for #SXSW can get their new iPads. {link} #wow\n",
       "103                                                 Know that &quot;dataviz&quot; translates to &quot;satanic&quot; on an iPhone. I'm just sayin'. #sxsw\n",
       "105              .@mention &quot;Google launched checkins a month ago.&quot; Check ins are ok, but CHECK OUTS are the future. #sxsw #Bizzy (via @mention\n",
       "107      Attending &quot;left brain search = Google, Right brain search = X&quot;  #Bettersearch  -- talking about the future of search engines at #sxsw\n",
       "108                   #HP opens &quot;Mobile Park&quot;  &amp; Content Incubator at #SXSW  {link}   #Apple  constructs  &quot;pop-up&quot; store  {link}\n",
       "110                                                      Kawasaki: &quot;pagemaker saved Apple.&quot; Oh those were the days. #sxsw #jwtatl #enchantment\n",
       "112                                                                Spark for #android is up for a #teamandroid award at #SXSW read about it here: {link}\n",
       "113                                                                                            Unboxing. #Apple #sxsw  @mention Apple Store, SXSW {link}\n",
       "115                                                                           At #SXSW, #Apple schools the #marketing experts | SXSW - CNET Blogs {link}\n",
       "117                                                                                                At #SXSW, #Apple schools the marketing experts {link}\n",
       "122                                        Headed to #Austin for #SXSW? Check out my map for newbies {link} @mention @mention , @mention @mention Enjoy!\n",
       "123                      Funny how #Austin is trending but not #SXSW. Only a matter of minutes at this point (at least according to Twitter for iPhone).\n",
       "125           #sxsw #ux #ipad #uxdes remember to ultimately be aware of the audience your app is targeted towards. An unexpected experience can be good.\n",
       "129                                              #Google's #Mobile Future, and the Elusive 'Power of Here' - {link} (via @mention #eurorscg #sxsw #sxswi\n",
       "130                                                              For those #notatSXSW (or at #SXSW), here's {link} Free to download and meet nearby peps\n",
       "131                                               Does your #SmallBiz need reviews to play on Google Places...We got an App for that..{link}  #seo #sxsw\n",
       "132                                                Does your #SmallBiz need reviews to play on Google Places...We got an App for that..{link} #seo #sxsw\n",
       "133                                                                           #Samsung, #Sony follow #Apple, #HP lead @mention {link} #Austin #atx #SXSW\n",
       "134                                                         #Samsung, #Sony follow #Apple, #HP lead @mention {link} #Austin #atx #SXSW /via @mention ^rg\n",
       "137             Q1 Was at #sxsw #sxswi for prep. Amazing pre push locally. Focus on location based. Google owns 10% of the regions billboards. #pr20chat\n",
       "138                                                                            Any other #Sxsw accounts I need to follow or apps to download for iPhone?\n",
       "139                                 Headed to #sxsw and want to share/gather contact info? {link} can turn your iphone into a business card broadcaster.\n",
       "140                                                  Headed to #sxsw and want to share/gather contact info? {link} can turn your iphone into a... {link}\n",
       "141                                                                       BTW - The #sxsw Apple store is sold out of all 3G models (VZW &amp; AT&amp;T).\n",
       "144                                                     Anyone at #sxsw been by the pop-up Apple store in Austin? That's gotta be a hopping place today.\n",
       "147       #fastball #sxsw Giving away two NEW Ipad2 wifi 32g black Apple cover tweet @mention fo more info #sxswi #attsxsw  Tonight @mention bo.lt house\n",
       "148                    Anyone at #sxsw had a chance to check out the pop-up Apple store? Wondering if it is worth the trek from the convention center...\n",
       "152                                                  @mention  #SXSW is an Austin conference. Do a google search - they have interactive / music / film.\n",
       "153                                                                                  Anyone at #sxsw know if apple will be (or is) selling ipad 2 there?\n",
       "154                                                                         Anyone at #SXSW know if the apple store has had a new shipment of iPads yet?\n",
       "155                                  Marc Ecko #SXSW launches #iPhone app. to autodial political change! {link} #edreform #edtech #eduVC #FightThePaddle\n",
       "157                                                  @mention  #SXSW LonelyPlanet Austin guide for #iPhone is free for a limited time {link} #lp #travel\n",
       "158                                                                                         More free #SXSW mp3 downloads, this time from iTunes: {link}\n",
       "159                                                                    Anyone at #sxsw or heading to aclu event seen owt to do with google circles then?\n",
       "160                                                                    @mention  #SXSW prompt for memory: go to Google map and describe a childhood walk\n",
       "162                                                                                                                        Essential #sxsw tools: {link}\n",
       "164           Following #sxsw Tweets on Google Realtime, four platforms on Tweet Deck and listening to panel, realizing I'm spoken to no one here today.\n",
       "165                          Anyone at #sxsw want an iPad 2? I'm in line and will pick one up for someone willing to pay me 50 for me to grab 1 for you?\n",
       "167                                                Solving a #SXSW-induced iPhone-in-toilet crisis at Apple Store with @mention (not my crisis for once)\n",
       "169                                                             Attending #sxsw? Austin Guide by @mention is now free to download on iTunes - {link} #lp\n",
       "175                                                        Hey #sxsw #sxswi folks. If you want to learn about security come over to #bsidesaustin {link}\n",
       "176             Attending #SXSWi? Work in iPhone / iPad game development? Looking to hire an Austin-based iOS developer? I'm your man. Let's talk. #SXSW\n",
       "178                                                   GSD&amp;M + Google 7-10. RT @mention What's the best party to hit tonight? #sxsw @mention @mention\n",
       "179                                                                           GSD&amp;M + Google Industry Party #SXSW @mention great to meet you  {link}\n",
       "181                                             #sxsw day 1 - Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile {link}\n",
       "184                                                                                       .@mention 1154 free songs from #SXSW (this year alone!) {link}\n",
       "185                                                                                  more than 150 million mobile users for Google Maps for Mobile #SXSW\n",
       "186                                                                             Currently 150 people in line at the &quot;Pop Up Apple Store&quot; #sxsw\n",
       "187                              Only iPad 2 available at #sxsw is the 64GB wifi-only model at $699, plus the optional (not!) leather smart cover at $69\n",
       "188                      ÂÃ·Â¼ We love 2 entertain youÂÃ_Please donÂÃÂªt be grateful! ÂÃ·_ {link} ÂÃ£_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter\n",
       "189                                                                 Less than 2 hours until we announce the details on the iPad 2 giveaway! #SXSW #SXSWi\n",
       "191                                                                                                                 (The iPad 2 queue at #sxsw of course\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df[df['Brand'].isna()]\n",
    "\n",
    "# Select the first 100 lines of column A from the filtered DataFrame\n",
    "column_a_subset = filtered_df['Tweet']\n",
    "column_a_subset[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are indeed words in the tweets that will let us identify the brand from the comment. I will create a list of keywords and map them to the different brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign rows to Brand/Product for the unknown one\n",
    "\n",
    "keywords = ['google', 'apple', 'ipad', 'android', 'iphone']\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['Tweet']\n",
    "    if pd.isna(row['Brand/Product']) and isinstance(text, str):\n",
    "        for keyword in keywords:\n",
    "            if keyword in text.lower():\n",
    "                df.at[index, 'Brand/Product'] = keyword\n",
    "                break\n",
    "# fill the rest with Unknown\n",
    "df['Brand/Product'] = df['Brand/Product'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_keywords = {\n",
    "    \"Apple\": [\"ipad\", \"iphone\", \"itunes\", \"apple\"],\n",
    "    \"Google\": [\"android\", \"google\"]\n",
    "}\n",
    "\n",
    "# Iterate over the DataFrame and update the 'Brand' column for tweets with missing brand information\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['Brand']):  # Check if the brand is NaN\n",
    "        tweet = row['Tweet']\n",
    "        if isinstance(tweet, str):  # Check if the tweet is a string\n",
    "            tweet = tweet.lower()  # Transform the tweet to lowercase\n",
    "            for brand, keywords in brand_keywords.items():\n",
    "                for keyword in keywords:\n",
    "                    if keyword in tweet:\n",
    "                        df.at[index, 'Brand'] = brand\n",
    "                        break  # Break the loop if a matching keyword is found\n",
    "        else:\n",
    "            df.at[index, 'Brand'] = 'unknown'  # Assign 'unknown' for NaN values in 'Brand' column\n",
    "\n",
    "# Assign 'unknown' for any remaining NaN values in 'Brand' column\n",
    "df['Brand'].fillna('unknown', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple      5401\n",
       "Google     2985\n",
       "unknown     707\n",
       "Name: Brand, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Brand\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google                             1740\n",
       "apple                              1195\n",
       "ipad                               1069\n",
       "iPad                                946\n",
       "Unknown                             762\n",
       "iphone                              710\n",
       "Apple                               661\n",
       "iPad or iPhone App                  470\n",
       "Google                              430\n",
       "android                             326\n",
       "iPhone                              297\n",
       "Other Google product or service     293\n",
       "Android App                          81\n",
       "Android                              78\n",
       "Other Apple product or service       35\n",
       "Name: Brand/Product, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Brand/Product\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do a bit of clean up because of lower case and upper case values. I will map the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google                             2170\n",
      "iPad                               2015\n",
      "Apple                              1856\n",
      "iPhone                             1007\n",
      "Unknown                             762\n",
      "iPad or iPhone App                  470\n",
      "Android                             404\n",
      "Other Google product or service     293\n",
      "Android App                          81\n",
      "Other Apple product or service       35\n",
      "Name: Brand/Product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "product_mapping = {\n",
    "    \"google\": \"Google\",\n",
    "    \"apple\": \"Apple\",\n",
    "    \"ipad\": \"iPad\",\n",
    "    \"iphone\": \"iPhone\",\n",
    "    \"android\": \"Android\"\n",
    "}\n",
    "df[\"Brand/Product\"] = df[\"Brand/Product\"].replace(product_mapping)\n",
    "print(df[\"Brand/Product\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I want to check for duplicates or missing values and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal:  False    9071\n",
      "True       22\n",
      "dtype: int64\n",
      "After removal:  False    9071\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Before removal: \", df.duplicated().value_counts())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"After removal: \",df.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9071 entries, 0 to 9092\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          9070 non-null   object\n",
      " 1   Brand/Product  9071 non-null   object\n",
      " 2   Emotion        9071 non-null   object\n",
      " 3   Brand          9071 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 354.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#check for missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tweet Brand/Product                             Emotion    Brand\n",
      "6   NaN       Unknown  No emotion toward brand or product  unknown\n"
     ]
    }
   ],
   "source": [
    "na_tweets = df[df['Tweet'].isna()]\n",
    "\n",
    "# Print the rows where 'Tweet' is NaN\n",
    "print(na_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9070 entries, 0 to 9092\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet          9070 non-null   object\n",
      " 1   Brand/Product  9070 non-null   object\n",
      " 2   Emotion        9070 non-null   object\n",
      " 3   Brand          9070 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 354.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remove the rows where 'Tweet' is NaN\n",
    "df = df.dropna(subset=['Tweet'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to clean up the Emotion column. There are 4 different options: Positive emotion, negative emotion, no emotion and can't tell. The \"no emotion\" option is the most common one and since I want to create a binary classifier, I will leave positive as positive and combine the neutral and negative ones as non-positive. Also,  I will  drop the can't tell rows as they are only a very small fraction of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5375\n",
       "Positive emotion                      2970\n",
       "Negative emotion                       569\n",
       "I can't tell                           156\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Non-positive    0.666816\n",
       "Positive        0.333184\n",
       "Name: Emotion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#emotions dictionary for mapping\n",
    "emotions = {\n",
    "    \"No emotion toward brand or product\": \"Non-positive\",\n",
    "    \"Positive emotion\": \"Positive\",\n",
    "    \"Negative emotion\": \"Non-positive\"\n",
    "}\n",
    "#mapping old labels to new ones\n",
    "df[\"Emotion\"] = df[\"Emotion\"].map(emotions)\n",
    "#check for nas and drop them (can't tell)\n",
    "print(df['Emotion'].isnull().sum())\n",
    "# Drop NaN in the emotion column\n",
    "df.dropna(subset = [\"Emotion\"], inplace = True)\n",
    "#check for distribution\n",
    "df[\"Emotion\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing to numerical\n",
    "df[\"Emotion\"] = df[\"Emotion\"].map({'Non-positive': 0, 'Positive': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my future target variable, I can note that I have a class imbalance. 66% of the cases are not positive, so I will perform different oversampling or undersampling techniques after the train test split to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Before I start with splitting my dataset and doing the preprocessing, I want to get more familiar with the most frequent words.  In this section, I will see how my tokens look like so that I can remove certain stop words in my model.\n",
    "\n",
    "## Basic preprocessing\n",
    "\n",
    "Now that I have my dataframe cleaned up, I will start with preparing the tweet texts. Here are the decisions, I have taken:\n",
    "\n",
    "Stop word removal: I will remove some basic stop words and use TF-IDF to apply weighting\n",
    "\n",
    "Stemming or Lemmatization: In my basic model, I will try stemming and later try another model with lemmatization.\n",
    "\n",
    "Tokenization: I will use a specific Tweet Tokenizer that handles hashtags and mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenization using TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#sxsw', 8916), ('.', 5771), ('the', 4346), ('link', 4249), ('}', 4234), ('{', 4231), ('to', 3529), (',', 3459), ('at', 3045), ('rt', 2918), ('for', 2503), ('ipad', 2367), ('!', 2338), ('a', 2295), ('google', 2082), ('in', 1887), (':', 1793), ('apple', 1778), ('of', 1676), ('is', 1668)]\n"
     ]
    }
   ],
   "source": [
    "#initialising Tokenizer \n",
    "tknzr = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
    "df['Tokens'] = df['Tweet'].apply(tknzr.tokenize)\n",
    "\n",
    "#writing a function to get the 20 most common words\n",
    "def get_most_common_words(df, column_name, N=20):\n",
    "    # Flatten the list of tokens into a single list\n",
    "    all_tokens = [token for tokens in df[column_name] for token in tokens]\n",
    "\n",
    "    # Calculate the frequency distribution\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "    # Get the top N common words\n",
    "    most_common_words = freq_dist.most_common(N)\n",
    "\n",
    "    return most_common_words\n",
    "\n",
    "# applying the function\n",
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list of most common words, there are a lot of common words / stopwords included that I will get rid of. Also, I will include \"sxsw\" which is an acronym for the Southwest Bank and \"rt\" which probably stands for retweet to my list of stopwords.I will first remove the stopwords and then see what else I can remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 5771), ('link', 4249), ('}', 4234), ('{', 4231), (',', 3459), ('ipad', 2367), ('!', 2338), ('google', 2082), (':', 1793), ('apple', 1778), ('\"', 1657), ('?', 1572), ('store', 1455), ('2', 1305), ('iphone', 1278), ('-', 1146), ('new', 1073), ('austin', 829), ('&', 827), ('app', 793)]\n"
     ]
    }
   ],
   "source": [
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = [\"#sxsw\", \"sxsw\", \"sxswi\", \"#sxswi\", \"rt\"]\n",
    "stop_words.update(additional_stopwords)\n",
    "\n",
    "# Function to remove stopwords from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tokens' column\n",
    "df['Tokens'] = df['Tokens'].apply(remove_stopwords)\n",
    "#get the top 20 words\n",
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also remove the product specific words and treat them as stopwords. I have this information already in my brand and product column, so I know which Apple or Google product the tweet is about. Also, I will remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 1073), ('app', 793), ('launch', 633), ('social', 612), ('today', 560), ('circles', 552), ('network', 451), ('via', 428), ('pop-up', 411), ('line', 393), ('get', 391), ('free', 368), ('party', 347), ('called', 347), ('mobile', 305), ('major', 296), ('like', 280), ('one', 271), ('time', 269), ('temporary', 262)]\n"
     ]
    }
   ],
   "source": [
    "additional_stopwords = [\n",
    "    \"ipad\", \"google\", \"apple\", \"iphone\", \"amp\",\n",
    "    \"android\", \"sxswi\", \"link\", \"#apple\",\n",
    "    \"#google\", \"...\", \"\\x89\", \"#ipad2\",\n",
    "    \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\n",
    "    \"#iphone\", \"#android\", \"store\", \"austin\", \"#ipad\"]\n",
    "stop_words.update(additional_stopwords)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tokens' column\n",
    "df['Tokens'] = df['Tokens'].apply(remove_stopwords)\n",
    "# Remove punctuation from the tokens\n",
    "df['Tokens'] = df['Tokens'].apply(lambda tokens: [token for token in tokens if token not in string.punctuation])\n",
    "#get most common words\n",
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check most common words for Apple and Google products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting up data in brands and emotions\n",
    "apple = df[df[\"Brand\"]==\"Apple\"]\n",
    "apple_pos = apple[apple[\"Emotion\"]==1]\n",
    "apple_nonpos = apple[apple[\"Emotion\"]==0]\n",
    "google = df[df[\"Brand\"]==\"Google\"]\n",
    "google_pos = google[google[\"Emotion\"]==1]\n",
    "google_nonpos = google[google[\"Emotion\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('app', 309), ('new', 219), ('pop-up', 151), ('line', 123), ('get', 120), ('via', 103), ('one', 98), (\"i'm\", 96), ('cool', 96), ('temporary', 89), ('free', 88), ('opening', 87), ('downtown', 86), ('like', 81), ('go', 79), ('launch', 78), ('time', 78), ('great', 77), ('popup', 76), ('day', 73)]\n"
     ]
    }
   ],
   "source": [
    "top_words = get_most_common_words(apple_pos, \"Tokens\", N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('app', 286), ('pop-up', 259), ('new', 257), ('line', 250), ('temporary', 172), ('opening', 166), ('get', 147), ('via', 137), ('free', 131), ('downtown', 130), ('popup', 129), ('open', 129), ('one', 121), ('launch', 118), ('pop', 102), ('like', 99), ('need', 96), (\"i'm\", 95), ('win', 93), ('people', 92)]\n"
     ]
    }
   ],
   "source": [
    "top_words = get_most_common_words(apple_nonpos, \"Tokens\", N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 139), ('party', 105), ('circles', 105), ('social', 103), ('maps', 101), ('network', 84), ('launch', 81), ('mobile', 73), ('app', 72), ('mayer', 64), ('today', 63), ('called', 60), ('great', 59), ('marissa', 59), (\"google's\", 56), ('major', 54), ('time', 49), ('w', 41), ('possibly', 41), ('get', 38)]\n"
     ]
    }
   ],
   "source": [
    "top_words = get_most_common_words(google_pos, \"Tokens\", N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('social', 439), ('circles', 433), ('new', 420), ('network', 345), ('launch', 331), ('today', 316), ('called', 262), ('major', 228), ('possibly', 187), ('mobile', 166), ('party', 140), ('via', 128), ('mayer', 124), (\"google's\", 121), ('marissa', 117), ('maps', 106), ('app', 86), ('#circles', 79), ('search', 77), ('bing', 69)]\n"
     ]
    }
   ],
   "source": [
    "top_words = get_most_common_words(google_nonpos, \"Tokens\", N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with a baseline model\n",
    "My steps for modeling are the following:\n",
    "1. Removing stop-words \n",
    "2. Train-Test-Split\n",
    "3. Address class imbalance\n",
    "4. Build and train baseline model with basic preprocessing and a vectorizer\n",
    "5. Evaluate the baseline model\n",
    "6. Finetune the preprocessing\n",
    "7. Potentially include other features\n",
    "8. Iterate through different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Removing stopwords\n",
    "Based on my EDA, I will remove a specific list of stopwords that has to do with the sxsw festival and product related words that won't have a lot of value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write function to remove stopwords\n",
    "def remove_stopwords(tweet):\n",
    "    stop_words = set(stopwords.words('english')) #basic stopwords\n",
    "    additional_stopwords = [\n",
    "        \"#sxsw\", \"sxsw\", \"sxswi\", \"#sxswi\", \"rt\",\"ipad\",\n",
    "        \"google\", \"apple\", \"iphone\", \"amp\",\n",
    "        \"android\", \"sxswi\", \"link\", \"#apple\",\n",
    "        \"#google\", \"...\", \"\\x89\", \"#ipad2\",\n",
    "        \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\n",
    "        \"#iphone\", \"#android\", \"store\", \"austin\", \"#ipad\"\n",
    "    ] + list(string.punctuation) \n",
    "    stop_words.update(additional_stopwords)\n",
    "    \n",
    "    filtered_tweet = ' '.join([word for word in tweet.split() if word.lower() not in stop_words])\n",
    "    return filtered_tweet\n",
    "# add column with filtered tweets\n",
    "df[\"Tweets_filtered\"] = df[\"Tweet\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train-Test-Split\n",
    "To avoid data leakage, I will now split my cleaned dataset into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (6685, 1) (6685,)\n",
      "Testing data shape: (2229, 1) (2229,)\n",
      "Training data shape: (6685,) (6685,)\n",
      "Testing data shape: (2229,) (2229,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X = df[['Tweets_filtered']]  # Feature\n",
    "y = df['Emotion']  # Target variable\n",
    "\n",
    "# Split the data into 75% training and 25% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape, y_test.shape)\n",
    "# Reshape X_train and X_test\n",
    "X_train = X_train.values.ravel()\n",
    "X_test = X_test.squeeze()\n",
    "# Print the updated shapes\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building a base model\n",
    "I will now build a logistic regression model with random oversampling and Count vectorization but without applying a specific tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                ('oversample', RandomOverSampler(random_state=42)),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('oversample', RandomOverSampler(random_state=42)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the resampled training data\n",
    "pipe_lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79      1490\n",
      "           1       0.58      0.59      0.59       739\n",
      "\n",
      "    accuracy                           0.72      2229\n",
      "   macro avg       0.69      0.69      0.69      2229\n",
      "weighted avg       0.72      0.72      0.72      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report of the linear regression model indicates moderate performance. Precision was higher for class 0 (0.80) compared to class 1 (0.58), same as recall which was higher for class 0 (0.78) compared to class 1 (0.60). The F1-score was also higher for class 0 (0.79) compared to class 1 (0.59). The overall accuracy of the model was 0.78. \n",
    "\n",
    "Next, I will try the same model but with TweetTokenizer. Based on my EDA, I will do tokenization by using TweetTokenizer that handles hashtags and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x0000020D04629970>>)),\n",
       "                ('oversample', RandomOverSampler(random_state=42)),\n",
       "                ('lr', LogisticRegression(random_state=42))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tweet tokenizer to later include in the pipeline\n",
    "tokenizer = TweetTokenizer()\n",
    "lr_pipe_tknzr = Pipeline([('vectorizer', CountVectorizer(tokenizer=tokenizer.tokenize)),\n",
    "                    ('oversample', RandomOverSampler(random_state=42)),\n",
    "                    ('lr', LogisticRegression(random_state=42))])\n",
    "\n",
    "lr_pipe_tknzr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      1490\n",
      "           1       0.60      0.62      0.61       739\n",
      "\n",
      "    accuracy                           0.74      2229\n",
      "   macro avg       0.70      0.71      0.70      2229\n",
      "weighted avg       0.74      0.74      0.74      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = lr_pipe_tknzr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This slightly improved the model. I will try a different vectorization (TFIDF) to see if that improves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x0000020D04629970>>)),\n",
       "                ('oversample', RandomOverSampler(random_state=42)),\n",
       "                ('lr', LogisticRegression(random_state=42))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipe_tknzr_oversample = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize)),\n",
    "    ('oversample', RandomOverSampler(random_state=42)),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "lr_pipe_tknzr_oversample.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.78      1490\n",
      "           1       0.57      0.63      0.60       739\n",
      "\n",
      "    accuracy                           0.72      2229\n",
      "   macro avg       0.69      0.70      0.69      2229\n",
      "weighted avg       0.73      0.72      0.72      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = lr_pipe_tknzr_oversample.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is worse than before. I will go back to CountVectorizer but try a different classifier (random forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x0000020D04629970>>)),\n",
       "                ('undersample', RandomUnderSampler(random_state=42)),\n",
       "                ('rfc', RandomForestClassifier(random_state=42))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline with Random Forest\n",
    "rfc_pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=tokenizer.tokenize)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42)),\n",
    "    ('rfc', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "rfc_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.71      0.76      1490\n",
      "           1       0.54      0.68      0.60       739\n",
      "\n",
      "    accuracy                           0.70      2229\n",
      "   macro avg       0.68      0.69      0.68      2229\n",
      "weighted avg       0.72      0.70      0.71      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = rfc_pipe.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will try a GridSearch to tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\mullerju\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7388963660834454\n",
      "Best Hyperparameters: {'lr__C': 1, 'lr__max_iter': 100, 'lr__solver': 'liblinear', 'vectorizer__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tweet tokenizer to later include in the pipeline\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Define the pipeline\n",
    "lr_pipe_grid = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=tokenizer.tokenize, ngram_range=(1, 3))),\n",
    "    ('oversample', RandomOverSampler(random_state=42)),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'lr__C': [0.1, 1, 10],\n",
    "    'lr__solver': ['liblinear', 'sag', 'saga'],\n",
    "    'lr__max_iter': [100, 1000, 10000]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(lr_pipe_grid, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: The RFC achieved an accuracy of 0.73, indicating that 73% of the predictions were correct. This is better than the previous model.\n",
    "\n",
    "Class 0 (majority class) had a precision of 0.76, recall of 0.87, and an F1-score of 0.81. This means that the RFC performed relatively well in identifying class 0 instances.\n",
    "\n",
    "Class 1 (minority class) had a lower precision of 0.63, recall of 0.45, and an F1-score of 0.53. The RFC had more difficulty correctly identifying class 1 instances.\n",
    "\n",
    "The macro average F1-score, which considers the average performance across both classes, was 0.67. The weighted average F1-score, which accounts for class imbalances, was slightly higher at 0.72.\n",
    "\n",
    "In summary, while the RFC achieved a reasonable accuracy, it struggled to correctly classify instances of the minority class (class 1) compared to the majority class (class 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(stop_words={'!', '\"', '#', '#apple', '#google',\n",
       "                                             '#sxsw', '#sxswi', '$', '%', '&',\n",
       "                                             \"'\", '(', ')', '*', '+', ',', '-',\n",
       "                                             '.', '...', '/', '0', '1', '2',\n",
       "                                             '3', '4', '5', '6', '7', '8', '9', ...},\n",
       "                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x00000127FF741820>>)),\n",
       "                ('undersample', RandomUnderSampler(random_state=42)),\n",
       "                ('rfc',\n",
       "                 RandomForestClassifier(class_weight={0: 0.7, 1: 0.3},\n",
       "                                        random_state=42))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_pipe_undersample_weighted = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize, stop_words=stop_words)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42)),\n",
    "    ('rfc', RandomForestClassifier(class_weight={0: 0.7, 1: 0.3}, random_state=42))\n",
    "])\n",
    "\n",
    "rfc_pipe_undersample_weighted.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.73      0.76      1490\n",
      "           1       0.53      0.62      0.57       739\n",
      "\n",
      "    accuracy                           0.69      2229\n",
      "   macro avg       0.66      0.67      0.67      2229\n",
      "weighted avg       0.71      0.69      0.70      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = rfc_pipe_undersample_weighted.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(stop_words={'#android', '#apple', '#google',\n",
       "                                             '#ipad', '#ipad2', '#iphone',\n",
       "                                             '#sxsw', '#sxswi', '...', '0', '1',\n",
       "                                             '2', '3', '4', '5', '6', '7', '8',\n",
       "                                             '9', 'a', 'about', 'above',\n",
       "                                             'after', 'again', 'against', 'ain',\n",
       "                                             'all', 'am', 'amp', 'an', ...},\n",
       "                                 tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x0000020D04629970>>)),\n",
       "                ('smote', SMOTE(random_state=42, sampling_strategy=1)),\n",
       "                ('svm', SVC(random_state=42))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline with Naive Bayes\n",
    "svm_pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stop_words)),\n",
    "    ('smote', SMOTE(sampling_strategy=1, random_state=42)),\n",
    "    ('svm', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "svm_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.78      1490\n",
      "           1       0.56      0.52      0.54       739\n",
      "\n",
      "    accuracy                           0.71      2229\n",
      "   macro avg       0.67      0.66      0.66      2229\n",
      "weighted avg       0.70      0.71      0.70      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = svm_pipe.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Baseline model\n",
    "I will do TF-IDF vectorization as my features for the baseline model. I will start with a simple logistic regression model and then continue to iterate after evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will build a baseline logistic regression model with TF-IDF vectorization. In later models, I will remove stopwords and do lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6685,)\n",
      "(6685, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape X_train_resampled and X_test\n",
    "X_train_resampled = X_train_resampled.values.ravel()\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_resampled)\n",
    "\n",
    "# Reshape X_test\n",
    "X_test = X_test.squeeze()\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will evaluate the baseline model by calculating the accuracy and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.78      1490\n",
      "           1       0.56      0.64      0.60       739\n",
      "\n",
      "    accuracy                           0.71      2229\n",
      "   macro avg       0.68      0.69      0.69      2229\n",
      "weighted avg       0.72      0.71      0.72      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report of the linear regression model indicates moderate performance. Precision was higher for class 0 (0.82) compared to class 1 (0.50), while recall was higher for class 1 (0.70) compared to class 0 (0.66). The F1-score was also higher for class 0 (0.73) compared to class 1 (0.59). The overall accuracy of the model was 0.67. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.69      0.75      1490\n",
      "           1       0.53      0.70      0.60       739\n",
      "\n",
      "    accuracy                           0.69      2229\n",
      "   macro avg       0.67      0.69      0.67      2229\n",
      "weighted avg       0.72      0.69      0.70      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape X_test\n",
    "X_test = X_test.squeeze()\n",
    "# predict target with model on testing set\n",
    "y_pred = lr_pipe.predict(X_test)\n",
    "# print a classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report of the basic logistic regression model states an accuracy of .69. This means that 69% of the instances in the test set were correctly classified. The precision score indicates that the model is rather good at predicting the non-positive cases but not the positive ones. \n",
    "Based on my EDA (other notebook), I will do tokenization by using TweetTokenizer that handles hashtags and mentions and remove stop words (the product names, often ocurring mentions without value (sxsw, rt)). \n",
    "First, I will instantiate my tokenizer and then define my list of stopwords so that I can include it in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.69      0.74      1490\n",
      "           1       0.52      0.68      0.59       739\n",
      "\n",
      "    accuracy                           0.68      2229\n",
      "   macro avg       0.67      0.68      0.67      2229\n",
      "weighted avg       0.72      0.68      0.69      2229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text using NLP techniques\n",
    "# Convert X_train and X_test back to pandas Series\n",
    "X_train_resampled = pd.Series(X_train_resampled)\n",
    "X_test_resampled = pd.Series(X_test)\n",
    "#initialising Tokenizer \n",
    "tokenizer = TweetTokenizer(preserve_case=False)\n",
    "#defining stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = [\n",
    "    \"#sxsw\", \"sxsw\", \"sxswi\", \"#sxswi\", \"rt\",\"ipad\",\n",
    "    \"google\", \"apple\", \"iphone\", \"amp\",\n",
    "    \"android\", \"sxswi\", \"link\", \"#apple\",\n",
    "    \"#google\", \"...\",\n",
    "    \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"\n",
    "] + list(string.punctuation) \n",
    "stop_words.update(additional_stopwords)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization using TweetTokenizer\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing to the training data\n",
    "X_train_preprocessed = X_train_resampled.apply(preprocess_text)\n",
    "\n",
    "# Apply preprocessing to the test data\n",
    "X_test_preprocessed = X_test.apply(preprocess_text)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_preprocessed)\n",
    "\n",
    "# Transform the preprocessed test data using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train_resampled)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my dataframe cleaned up, I will start with preparing the tweet texts. Here are the decisions, I have taken:\n",
    "\n",
    "Stop word removal: I will remove some basic stop words and use TF-IDF to apply weighting\n",
    "\n",
    "Stemming or Lemmatization: In my basic model, I will try stemming and later try another model with lemmatization.\n",
    "\n",
    "Tokenization: I will use a specific Tweet Tokenizer that handles hashtags and mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenization using TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising Tokenizer \n",
    "tknzr = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
    "df['Tokens'] = df['Tweet'].apply(tknzr.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#sxsw', 8916), ('.', 5771), ('the', 4346), ('link', 4249), ('}', 4234), ('{', 4231), ('to', 3529), (',', 3459), ('at', 3045), ('rt', 2918), ('for', 2503), ('ipad', 2367), ('!', 2338), ('a', 2295), ('google', 2082), ('in', 1887), (':', 1793), ('apple', 1778), ('of', 1676), ('is', 1668)]\n"
     ]
    }
   ],
   "source": [
    "#initialising Tokenizer \n",
    "tknzr = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
    "df['Tokens'] = df['Tweet'].apply(tknzr.tokenize)\n",
    "\n",
    "#writing a function to get the 20 most common words\n",
    "def get_most_common_words(df, column_name, N=20):\n",
    "    # Flatten the list of tokens into a single list\n",
    "    all_tokens = [token for tokens in df[column_name] for token in tokens]\n",
    "\n",
    "    # Calculate the frequency distribution\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "    # Get the top N common words\n",
    "    most_common_words = freq_dist.most_common(N)\n",
    "\n",
    "    return most_common_words\n",
    "\n",
    "# applying the function\n",
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list of most common words, there are a lot of common words / stopwords included that I will get rid of. Also, I will include \"sxsw\" which is an acronym for the Southwest Bank and \"rt\" which probably stands for retweet to my list of stopwords.I will first remove the stopwords and then see what else I can remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords = [\"#sxsw\", \"sxsw\", \"sxswi\", \"#sxswi\", \"rt\"]\n",
    "stop_words.update(additional_stopwords)\n",
    "\n",
    "# Function to remove stopwords from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tokens' column\n",
    "df['Tokens'] = df['Tokens'].apply(remove_stopwords)\n",
    "#get the top 20 words\n",
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 5771), ('link', 4249), ('}', 4234), ('{', 4231), (',', 3459), ('ipad', 2367), ('!', 2338), ('google', 2082), (':', 1793), ('apple', 1778), ('\"', 1657), ('?', 1572), ('store', 1455), ('2', 1305), ('iphone', 1278), ('-', 1146), ('new', 1073), ('austin', 829), ('&', 827), ('app', 793)]\n"
     ]
    }
   ],
   "source": [
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also remove the product specific words and treat them as stopwords. I have this information already in my brand and product column, so I know which Apple or Google product the tweet is about. Also, I will remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = [\n",
    "    \"ipad\", \"google\", \"apple\", \"iphone\", \"amp\",\n",
    "    \"android\", \"sxswi\", \"link\", \"#apple\",\n",
    "    \"#google\", \"...\",\n",
    "    \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "stop_words.update(additional_stopwords)\n",
    "\n",
    "# Apply the remove_stopwords function to the 'tokens' column\n",
    "df['Tokens'] = df['Tokens'].apply(remove_stopwords)\n",
    "# Remove punctuation from the tokens\n",
    "df['Tokens'] = df['Tokens'].apply(lambda tokens: [token for token in tokens if token not in string.punctuation])\n",
    "#get most common words\n",
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('store', 1455), ('2', 1305), ('new', 1073), ('austin', 829), ('app', 793), ('\\x89', 676), ('launch', 633), ('social', 612), ('today', 560), ('circles', 552), ('network', 451), ('via', 428), ('pop-up', 411), ('line', 393), ('get', 391), ('free', 368), ('party', 347), ('called', 347), ('mobile', 305), ('major', 296)]\n"
     ]
    }
   ],
   "source": [
    "top_words = get_most_common_words(df, 'Tokens', N=20)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lemmatized tokens back to text\n",
    "df['processed_text'] = df['Tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the processed text to obtain the TF-IDF features\n",
    "tfidf_features = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Get the feature names (words) corresponding to the columns in the TF-IDF matrix\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Convert the TF-IDF features to a DataFrame for further analysis\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>0310apple</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>Ã»Ã²</th>\n",
       "      <th>Ã»Ã²and</th>\n",
       "      <th>Ã»Ã³</th>\n",
       "      <th>Ã»Ã³can</th>\n",
       "      <th>Ã»Ã³just</th>\n",
       "      <th>Ã»Ã³lewis</th>\n",
       "      <th>Ã»Ã³lots</th>\n",
       "      <th>Ã»Ã³my</th>\n",
       "      <th>Ã»Ã³the</th>\n",
       "      <th>Ã¼_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 8836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   01   02   03  0310apple   06   08   10  100  ...   Ã»Ã²  Ã»Ã²and  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.0  ...  0.0    0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.0  ...  0.0    0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.0  ...  0.0    0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.0  ...  0.0    0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  0.0  ...  0.0    0.0   \n",
       "\n",
       "    Ã»Ã³  Ã»Ã³can  Ã»Ã³just  Ã»Ã³lewis  Ã»Ã³lots  Ã»Ã³my  Ã»Ã³the   Ã¼_  \n",
       "0  0.0    0.0     0.0      0.0     0.0   0.0    0.0  0.0  \n",
       "1  0.0    0.0     0.0      0.0     0.0   0.0    0.0  0.0  \n",
       "2  0.0    0.0     0.0      0.0     0.0   0.0    0.0  0.0  \n",
       "3  0.0    0.0     0.0      0.0     0.0   0.0    0.0  0.0  \n",
       "4  0.0    0.0     0.0      0.0     0.0   0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 8836 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
